#!/usr/bin/env python3
"""
Full Academic Papers TTS Generator

This script contains the complete academic paper presentations and can generate
full-length audio versions using multi-speaker TTS.

Usage:
    python full_papers_generator.py --paper [paper_name]
    python full_papers_generator.py --all
"""

import os
import argparse
from gemini_tts_example import text_to_speech_multi_speaker


# Full paper scripts
GNEISS_WEB_SCRIPT = """Narrator 1: Welcome to an overview of the paper "GneissWeb: Preparing High Quality Data for LLMs at Scale." This research from IBM explores a new, large-scale dataset designed to improve the performance of Large Language Models.

Narrator 2: That's right. The performance of LLMs is highly dependent on both the quantity and, crucially, the quality of the training data. While large pre-training datasets for leading LLMs aren't public, many open datasets are relatively small, often less than 5 trillion tokens.

Narrator 1: And that limits their suitability for training truly large models, especially for what's called Stage-1 long token horizon training.

Narrator 2: Exactly. This paper introduces GneissWeb, a substantial dataset yielding around 10 trillion tokens. The key is its recipe, designed to meet both the quality and quantity needs for training LLMs effectively.

Narrator 1: The GneissWeb recipe involves a couple of core techniques: sharded exact sub-string deduplication and a carefully built ensemble of quality filters.

Narrator 2: They found that models trained on GneissWeb outperform models trained on other state-of-the-art large open datasets, like FineWeb-V1.1.0.

Narrator 1: For example, on a set of 11 commonly used benchmarks, models trained on GneissWeb achieved an average score 2.73 percentage points higher than those trained on FineWeb-V1.1.0. Even when expanding to 20 benchmarks, GneissWeb still held a 1.75 percentage point advantage.

Narrator 2: The Introduction section really sets the stage, discussing how recent LLMs, like Llama3 and Gemma2, are trained on far more data than older scaling laws suggested.

Narrator 1: This highlights the need for massive datasets, but as we mentioned, public options over 5 trillion tokens are scarce. The paper notes that FineWeb is a sufficiently large starting point at 15 trillion tokens, but its quality filtering isn't tailored for the most effective Stage-1 training.

Narrator 2: GneissWeb aims to bridge this gap, distilling a high-quality subset from FineWeb while maintaining a large scale, around 10 trillion tokens."""


CODE_COMMENT_SCRIPT = """NARRATOR 1: Welcome to our exploration of the paper, "A ML-LLM Pairing for Better Code Comment Classification," authored by Hanna Abi Akl. This work was presented at the Information Retrieval in Software Engineering, or IRSE, shared task at FIRE 2023.

NARRATOR 2: This paper tackles a fascinating and challenging problem: determining whether a code comment is actually useful for understanding the relevant code snippet. It's a binary classification task â€“ useful or not useful.

NARRATOR 1: The authors approached this challenge with a two-fold strategy. First, they compared classical machine learning systems.

NARRATOR 2: And second, they looked at the data side, generating additional training data using large language models, or LLMs, specifically through prompting, to see if performance would increase.

NARRATOR 1: Their best result, using a Neural Network, achieved a Macro-F1 score of 88.401% on the initial dataset.

NARRATOR 2: And crucially, they saw a 1.5% overall increase in performance when they included the data generated by the LLM. This approach secured them second place in the shared task.

NARRATOR 1: The core of this research lies in Natural Language Processing, Machine Learning, Information Retrieval, Large Language Models, and their application to Code Comprehension and Comment Quality.

NARRATOR 2: Let's dive into the introduction. In software development, code and documentation are essential. Comments are a key form of documentation, inserted directly into the code in natural language. They help clarify logic without affecting performance."""


FINEWEB_SCRIPT = """Narrator 1: Welcome to our discussion on "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale." The performance of large language models, or LLMs, relies heavily on the quality and size of their pretraining datasets.

Narrator 2: Exactly. But the datasets used for top open LLMs like Llama 3 and Mixtral aren't public, and little is known about their creation process. This work introduces FineWeb, a massive 15-trillion token dataset.

Narrator 1: FineWeb is derived from 96 Common Crawl snapshots. It's designed to produce better-performing LLMs compared to other open datasets.

Narrator 2: A key contribution is documenting and exploring the design choices behind FineWeb, including deep dives into deduplication and filtering strategies through what they call "data ablations."

Narrator 1: They also present FineWeb-Edu, a 1.3-trillion token subset of FineWeb specifically filtered for educational text. Models trained on this subset show significantly improved performance on knowledge and reasoning benchmarks like MMLU and ARC.

Narrator 2: Crucially, they are releasing both datasets, their data curation codebase called datatrove, and the models trained during their ablation experiments, aiming to reduce the gap between proprietary and public knowledge in this field."""


DATACOMP_SCRIPT = """Narrator 1: Welcome to our summary of the paper "DataComp-LM: In search of the next generation of training sets for language models". This research introduces a new benchmark focused on dataset quality for training language models.

Narrator 2: Large language models have seen incredible progress, but their training relies on massive datasets, often web crawls. The cost of training is high, making efficient generalization crucial.

Narrator 1: A key challenge in dataset research is the lack of controlled comparisons. Different models, architectures, and compute budgets make it hard to isolate the impact of the training data itself.

Narrator 2: Additionally, training data details for even open-weight models are often scarce, making it difficult to understand what constitutes a state-of-the-art training set.

Narrator 1: To address this, the researchers introduce DataComp for Language Models, or DCLM. It's the first large-scale benchmark for language model training data curation.

Narrator 2: The core idea is for participants to propose new datasets or curation algorithms and then evaluate them by training language models using a fixed, standardized recipe."""


REFINED_WEB_SCRIPT = """Narrator 1: Welcome to an overview of the paper, "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only." This work challenges conventional wisdom in training large language models.

Narrator 2: Large language models, or LLMs, are typically trained on a mix of filtered web data and specially curated "high-quality" sources like books, social media, or technical papers. This curation has been thought essential for performance.

Narrator 1: However, as models grow and require pretraining on trillions of tokens, the scalability and availability of unique, curated data become major concerns.

Narrator 2: This paper proposes a different approach: showing that properly processed web data alone can be sufficient, even outperforming models trained on existing curated corpora like The Pile.

Narrator 1: They achieved this with a dataset called REFINEDWEB, derived from CommonCrawl. Despite extensive filtering, they obtained five trillion tokens.

Narrator 2: And importantly, they're publicly releasing a 600-billion token extract of RefinedWeb, alongside 1.3 and 7.5 billion parameter language models trained on it."""


PAPERS = {
    "gneiss_web": {
        "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale",
        "script": GNEISS_WEB_SCRIPT,
        "output": "gneiss_web_full.wav"
    },
    "code_comment": {
        "title": "A ML-LLM Pairing for Better Code Comment Classification",
        "script": CODE_COMMENT_SCRIPT,
        "output": "code_comment_full.wav"
    },
    "fineweb": {
        "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale",
        "script": FINEWEB_SCRIPT,
        "output": "fineweb_full.wav"
    },
    "datacomp_lm": {
        "title": "DataComp-LM: In search of the next generation of training sets for language models",
        "script": DATACOMP_SCRIPT,
        "output": "datacomp_lm_full.wav"
    },
    "refined_web": {
        "title": "The RefinedWeb Dataset for Falcon LLM",
        "script": REFINED_WEB_SCRIPT,
        "output": "refined_web_full.wav"
    }
}


def generate_paper_audio(paper_key):
    """Generate audio for a specific paper."""
    if paper_key not in PAPERS:
        print(f"âŒ Error: Paper '{paper_key}' not found!")
        print(f"Available papers: {', '.join(PAPERS.keys())}")
        return
    
    paper = PAPERS[paper_key]
    print(f"ğŸ“„ Generating: {paper['title']}")
    
    # Professional academic voices
    speakers = [
        {"name": "Narrator 1", "voice": "kore"},     # Professional, authoritative
        {"name": "Narrator 2", "voice": "charon"}   # Informative, clear
    ]
    
    # Add academic style instruction
    styled_script = f"Make both narrators sound professional and engaging, suitable for an academic presentation: {paper['script']}"
    
    try:
        text_to_speech_multi_speaker(styled_script, speakers, paper['output'])
        print(f"âœ… Generated: {paper['output']}")
        return True
    except Exception as e:
        print(f"âŒ Error generating {paper['output']}: {e}")
        return False


def generate_all_papers():
    """Generate audio for all papers."""
    print("ğŸ“ Generating Full Academic Paper Presentations")
    print("=" * 60)
    
    success_count = 0
    total_count = len(PAPERS)
    
    for paper_key in PAPERS.keys():
        if generate_paper_audio(paper_key):
            success_count += 1
        print()
    
    print(f"ğŸ‰ Completed: {success_count}/{total_count} papers generated successfully!")
    
    if success_count > 0:
        print("\nğŸ“‚ Generated files:")
        for paper in PAPERS.values():
            print(f"   â€¢ {paper['output']} - {paper['title']}")


def main():
    """Main function with command line arguments."""
    parser = argparse.ArgumentParser(description="Generate full academic paper TTS presentations")
    parser.add_argument("--paper", "-p", choices=list(PAPERS.keys()), 
                       help="Generate specific paper audio")
    parser.add_argument("--all", "-a", action="store_true", 
                       help="Generate all paper presentations")
    parser.add_argument("--list", "-l", action="store_true", 
                       help="List available papers")
    
    args = parser.parse_args()
    
    # Check if API key is available
    if not os.getenv("GEMINI_API_KEY"):
        print("âŒ Error: GEMINI_API_KEY environment variable not set!")
        print("Please set your API key and try again.")
        return
    
    if args.list:
        print("ğŸ“š Available Papers:")
        for key, paper in PAPERS.items():
            print(f"   â€¢ {key}: {paper['title']}")
        return
    
    if args.paper:
        generate_paper_audio(args.paper)
    elif args.all:
        generate_all_papers()
    else:
        print("ğŸ“ Full Academic Papers TTS Generator")
        print("Use --help for usage options")
        print("Examples:")
        print("  python full_papers_generator.py --all")
        print("  python full_papers_generator.py --paper gneiss_web")
        print("  python full_papers_generator.py --list")


if __name__ == "__main__":
    main() 