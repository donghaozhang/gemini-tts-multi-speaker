#!/usr/bin/env python3
"""
Academic Papers Multi-Speaker TTS Demo

This demo converts academic paper presentations into multi-speaker audio
using Google Gemini API. Each paper is presented by two narrators with
different voices to create engaging educational content.

Papers included:
1. GneissWeb: Preparing High Quality Data for LLMs at Scale
2. A ML-LLM Pairing for Better Code Comment Classification  
3. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale
4. DataComp-LM: In search of the next generation of training sets for language models
5. The RefinedWeb Dataset for Falcon LLM

Usage:
    python academic_papers_demo.py
"""

import os
from gemini_tts_example import text_to_speech_multi_speaker


def demo_gneiss_web():
    """Demo: GneissWeb paper presentation."""
    print("📄 Paper 1: GneissWeb - Preparing High Quality Data for LLMs at Scale")
    
    # Using a shorter excerpt for demo purposes - you can expand this
    dialogue = """Narrator 1: Welcome to an overview of the paper "GneissWeb: Preparing High Quality Data for LLMs at Scale." This research from IBM explores a new, large-scale dataset designed to improve the performance of Large Language Models.

Narrator 2: That's right. The performance of LLMs is highly dependent on both the quantity and, crucially, the quality of the training data. While large pre-training datasets for leading LLMs aren't public, many open datasets are relatively small, often less than 5 trillion tokens.

Narrator 1: And that limits their suitability for training truly large models, especially for what's called Stage-1 long token horizon training.

Narrator 2: Exactly. This paper introduces GneissWeb, a substantial dataset yielding around 10 trillion tokens. The key is its recipe, designed to meet both the quality and quantity needs for training LLMs effectively.

Narrator 1: The GneissWeb recipe involves a couple of core techniques: sharded exact sub-string deduplication and a carefully built ensemble of quality filters.

Narrator 2: They found that models trained on GneissWeb outperform models trained on other state-of-the-art large open datasets, like FineWeb-V1.1.0.

Narrator 1: For example, on a set of 11 commonly used benchmarks, models trained on GneissWeb achieved an average score 2.73 percentage points higher than those trained on FineWeb-V1.1.0."""
    
    speakers = [
        {"name": "Narrator 1", "voice": "kore"},     # Professional, authoritative
        {"name": "Narrator 2", "voice": "charon"}   # Informative, clear
    ]
    
    text_to_speech_multi_speaker(dialogue, speakers, "gneiss_web_paper.wav")
    print()


def demo_code_comment_classification():
    """Demo: ML-LLM Code Comment Classification paper."""
    print("📄 Paper 2: A ML-LLM Pairing for Better Code Comment Classification")
    
    dialogue = """NARRATOR 1: Welcome to our exploration of the paper, "A ML-LLM Pairing for Better Code Comment Classification," authored by Hanna Abi Akl. This work was presented at the Information Retrieval in Software Engineering, or IRSE, shared task at FIRE 2023.

NARRATOR 2: This paper tackles a fascinating and challenging problem: determining whether a code comment is actually useful for understanding the relevant code snippet. It's a binary classification task – useful or not useful.

NARRATOR 1: The authors approached this challenge with a two-fold strategy. First, they compared classical machine learning systems.

NARRATOR 2: And second, they looked at the data side, generating additional training data using large language models, or LLMs, specifically through prompting, to see if performance would increase.

NARRATOR 1: Their best result, using a Neural Network, achieved a Macro-F1 score of 88.401% on the initial dataset.

NARRATOR 2: And crucially, they saw a 1.5% overall increase in performance when they included the data generated by the LLM. This approach secured them second place in the shared task."""
    
    speakers = [
        {"name": "NARRATOR 1", "voice": "puck"},      # Upbeat, engaging
        {"name": "NARRATOR 2", "voice": "zephyr"}    # Bright, clear
    ]
    
    text_to_speech_multi_speaker(dialogue, speakers, "code_comment_paper.wav")
    print()


def demo_fineweb_datasets():
    """Demo: FineWeb Datasets paper."""
    print("📄 Paper 3: The FineWeb Datasets - Decanting the Web for the Finest Text Data")
    
    dialogue = """Narrator 1: Welcome to our discussion on "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale." The performance of large language models, or LLMs, relies heavily on the quality and size of their pretraining datasets.

Narrator 2: Exactly. But the datasets used for top open LLMs like Llama 3 and Mixtral aren't public, and little is known about their creation process. This work introduces FineWeb, a massive 15-trillion token dataset.

Narrator 1: FineWeb is derived from 96 Common Crawl snapshots. It's designed to produce better-performing LLMs compared to other open datasets.

Narrator 2: A key contribution is documenting and exploring the design choices behind FineWeb, including deep dives into deduplication and filtering strategies through what they call "data ablations."

Narrator 1: They also present FineWeb-Edu, a 1.3-trillion token subset of FineWeb specifically filtered for educational text. Models trained on this subset show significantly improved performance on knowledge and reasoning benchmarks like MMLU and ARC.

Narrator 2: Crucially, they are releasing both datasets, their data curation codebase called datatrove, and the models trained during their ablation experiments, aiming to reduce the gap between proprietary and public knowledge in this field."""
    
    speakers = [
        {"name": "Narrator 1", "voice": "aoede"},     # Breezy, natural
        {"name": "Narrator 2", "voice": "orus"}      # Firm, knowledgeable
    ]
    
    text_to_speech_multi_speaker(dialogue, speakers, "fineweb_paper.wav")
    print()


def demo_datacomp_lm():
    """Demo: DataComp-LM paper."""
    print("📄 Paper 4: DataComp-LM - In Search of the Next Generation of Training Sets")
    
    dialogue = """Narrator 1: Welcome to our summary of the paper "DataComp-LM: In search of the next generation of training sets for language models". This research introduces a new benchmark focused on dataset quality for training language models.

Narrator 2: Large language models have seen incredible progress, but their training relies on massive datasets, often web crawls. The cost of training is high, making efficient generalization crucial.

Narrator 1: A key challenge in dataset research is the lack of controlled comparisons. Different models, architectures, and compute budgets make it hard to isolate the impact of the training data itself.

Narrator 2: Additionally, training data details for even open-weight models are often scarce, making it difficult to understand what constitutes a state-of-the-art training set.

Narrator 1: To address this, the researchers introduce DataComp for Language Models, or DCLM. It's the first large-scale benchmark for language model training data curation.

Narrator 2: The core idea is for participants to propose new datasets or curation algorithms and then evaluate them by training language models using a fixed, standardized recipe."""
    
    speakers = [
        {"name": "Narrator 1", "voice": "leda"},      # Youthful, engaging
        {"name": "Narrator 2", "voice": "fenrir"}    # Excitable, enthusiastic
    ]
    
    text_to_speech_multi_speaker(dialogue, speakers, "datacomp_lm_paper.wav")
    print()


def demo_refined_web():
    """Demo: RefinedWeb Dataset paper."""
    print("📄 Paper 5: The RefinedWeb Dataset for Falcon LLM")
    
    dialogue = """Narrator 1: Welcome to an overview of the paper, "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only." This work challenges conventional wisdom in training large language models.

Narrator 2: Large language models, or LLMs, are typically trained on a mix of filtered web data and specially curated "high-quality" sources like books, social media, or technical papers. This curation has been thought essential for performance.

Narrator 1: However, as models grow and require pretraining on trillions of tokens, the scalability and availability of unique, curated data become major concerns.

Narrator 2: This paper proposes a different approach: showing that properly processed web data alone can be sufficient, even outperforming models trained on existing curated corpora like The Pile.

Narrator 1: They achieved this with a dataset called REFINEDWEB, derived from CommonCrawl. Despite extensive filtering, they obtained five trillion tokens.

Narrator 2: And importantly, they're publicly releasing a 600-billion token extract of RefinedWeb, alongside 1.3 and 7.5 billion parameter language models trained on it."""
    
    speakers = [
        {"name": "Narrator 1", "voice": "algieba"},    # Smooth, professional
        {"name": "Narrator 2", "voice": "schedar"}    # Even, balanced
    ]
    
    text_to_speech_multi_speaker(dialogue, speakers, "refined_web_paper.wav")
    print()


def create_full_paper_presentation(paper_name, full_script, output_file):
    """
    Create a full paper presentation from the complete script.
    
    Args:
        paper_name (str): Name of the paper
        full_script (str): Complete script text
        output_file (str): Output WAV filename
    """
    print(f"📄 Creating full presentation: {paper_name}")
    
    # Choose appropriate voices for academic presentation
    speakers = [
        {"name": "Narrator 1", "voice": "kore"},     # Professional
        {"name": "Narrator 2", "voice": "charon"}   # Informative
    ]
    
    # Add style instruction for academic tone
    styled_script = f"Make both narrators sound professional and informative, suitable for an academic presentation: {full_script}"
    
    text_to_speech_multi_speaker(styled_script, speakers, output_file)
    print(f"✅ Full presentation saved as: {output_file}")
    print()


def main():
    """Run academic papers multi-speaker TTS demo."""
    print("🎓 Academic Papers Multi-Speaker TTS Demo")
    print("=" * 60)
    print("Converting research paper presentations to audio...")
    print()
    
    # Check if API key is available
    if not os.getenv("GEMINI_API_KEY"):
        print("❌ Error: GEMINI_API_KEY environment variable not set!")
        print("Please set your API key and try again.")
        return
    
    try:
        # Run shorter demo versions of each paper
        demo_gneiss_web()
        demo_code_comment_classification()
        demo_fineweb_datasets()
        demo_datacomp_lm()
        demo_refined_web()
        
        print("🎉 Academic paper demos completed!")
        print("\n📂 Generated audio files:")
        print("   • gneiss_web_paper.wav - GneissWeb dataset research")
        print("   • code_comment_paper.wav - Code comment classification")
        print("   • fineweb_paper.wav - FineWeb datasets overview")
        print("   • datacomp_lm_paper.wav - DataComp-LM benchmark")
        print("   • refined_web_paper.wav - RefinedWeb dataset")
        
        print("\n💡 Applications for academic content:")
        print("   - Converting research papers to audio for accessibility")
        print("   - Creating podcast-style academic content")
        print("   - Generating audio summaries for literature reviews")
        print("   - Building educational audio materials")
        print("   - Making research more accessible while commuting")
        
        print("\n🔧 Customization options:")
        print("   - Adjust voice personalities for different presentation styles")
        print("   - Add emphasis and emotional tone for key findings")
        print("   - Create longer presentations with full paper content")
        print("   - Generate multiple language versions")
        
        # Offer to create full presentations
        print("\n📝 Note: This demo uses shortened excerpts.")
        print("To generate full paper presentations, use the complete scripts")
        print("provided in your input with the create_full_paper_presentation() function.")
        
    except Exception as e:
        print(f"❌ Error running academic papers demo: {e}")


if __name__ == "__main__":
    main() 